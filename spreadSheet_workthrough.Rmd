---
title: "Tidal Spreadsheet fun"
output: github_document

---
Take 2: 
After some digging into the task of extracting data from these spreadsheets, and the prospects of additional analyses (prioritization sensitivity, perhaps) as well as easing the management of field and desktop assessment needs, a new(ish) approach is laid out below. In general a reboot of the teams initial proposal of using a sort of look up key to identify the locations of important data from the spreadsheets.

To begin with, this process relies heavily on the tidyxl package, as well as the tidyverse packages more broadly. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

library(readr)
library(readxl)
library(tidyxl)
library(unpivotr)
library(tidyverse)

keysheet <- read_excel("spreadsheets/key.xlsx")

source("functions/culvert_tidy.R")
source("functions/culvert_extract.R")    

```

Using tidyxl's xlsx_cells() function we pull in all the cells from a given excel file. The cells arrive in a tidy data fashion where each row holds the data value, some formating information, data type, and the location in which it lives (the sheet and cell, specifically).  
Below is a snippet of a single sheet read into R using tidxl::xlsx_cells()

```{r}

# TODO Start with basic read in of spreadsheet and extracting the proper cells.
sheet <- "spreadsheets/Copy of Culvert37.xlsm" 
# Tidy up the sheets. 
cells <- xlsx_cells(sheet) %>% 
  filter(is_blank == FALSE, sheet != "Data Sheet - BLANK") 
  
cells

```

The next step is to scale this up to capture all the datasheets on hand. We do this by creating two functions that work together. The first, **culvert_fetch()**, fetches the tidyxl cells when given a file path name. The second function, **culvert_tidy**, produces a data frame containing all the files within the supplied folder, a few helpful bits of info and a column containing the tidyxl cells from the output of **culvert_fetch()** for each data sheet. 



```{r message=FALSE, warning=FALSE, paged.print=FALSE}
culvert_fetch

culvert_tidy


test1 <- culvert_tidy("spreadsheets/Ex")

glimpse(test1)

glimpse(test1$tidycells[[1]])

```

From this we can start extracting the parts we want from the data sheet. To ease that process we'll rely on a pair of functions and a **_key_** spreadsheet for guiding the extraction of the bits we want.
To follow the process behind these next few operations we need to understand how things are coming together here. Essentially each row in the tidy culvert data frame contains a column with all the data from the original tidal culvert field assessment datasheets. Given the complexity of these datasheets and the lack of consistent orientations between data values and data keys (or data and variable name if you will) each item of interest needs to be mappped out individually using a key that provides the location within that spreadsheet that that value of interest is found (which if luck is on our side is found consistently through out all these files...Should be...) and a name that we want to give that value. 
Once the key is built (note the code will run at anytime and return only those values that are mapped out in the key) it's fed into a function that will pull out the 'raw' data form it's in and pop it into an orderly, labelled, decoded sheet. And to ensure that nothing is mixed up in the process we'll add that new decoded data frame (or spreadsheet for sake of argument) as a new column to our already existing data frame 


#### Pros 
* More freedom and flexibility of up and down stream data management/analysis approaches, not limited to embedded excel formulas  
* If any sensitivity analysis or alterations to the prioritization calculations are to be explored those calculations can easily be altered; whereas now they exist in each individual excel file.
* At least in R, we can quickly find missing information from the 'raw' data sheet. Currently some formulas just yield more or less useless errors and are imported into R as '#Div/O!' 

#### Cons
* Will require a _fair amount_  of work re-coding Excel formulas into R code (or other)
  ** as mentioned above though most of the formulas are simple, single cell references, many even referencing cells that reference cells, that..etc. In fact of the 443 cells that contain some sort of formula only 139 of the cells contain references to more than one cell. And only a small proportion reference many cells (i.e. a bit messier / time consuming to dig into). In addition, many are simple _If(CELL = 0, " ", CELL)_- essentially changing just the way excel displays the information (blank vs 0)
  





*OR* Just focus on extracting the values of interest, most of which are in the **Data Sheet - SUMMARY** tab.   

#### Pros
* It might be less work, but not clear how much less... Potentially not a huge amount less.

#### Cons
Pretty much covered by my Pros list above for the other tactic. 

### Potential pifalls
There's a few issues (potential) that I've come across.  

* cells are merged which makes it hard to determine which cell actually contains the value of interest.  
* There are values that are selected using a formatted control (like a drop-down) which can be very easily altered.  
  + accessing these values is best done through the *Data Sheet - SUMMARY* tab.  
* There appear to be cells with just spaces possibly? They are showing up as blank, but NOT empty. I believe this is caused by IF(cell = 0, " ", cell) formulas.


## TO DOs:  
* Using a completed (and backed up) datasheet comb through the *spreadsheets/extractedKey_WORKING.xlsx* file to match data keys with data values.   

* **VERY IMPORTANT** work only in the 'extractedKey_WORKING' file. The other file will/can be overwritten.  

* Once this new master look-up/key is build here's the game plan.  
  + Create look-up in R using key-value pairs  built from the extractedKey file
  + within this look-up retain the 'expected keys' in a column to act as an error catcher on data extractions later.  
* Need to extract look-up tables in Lookup tab to form joins to link data value with data description (7 = Riprap under the Wingwall Materials dropdown.)  


```{r}

# Helper function to look up values in a cell of interest.
# 

get.cell.value <- function(tidysheet, cellOfInterest){
  
  val <- tidysheet %>% 
    filter(address == cellOfInterest) %>% 
    pull(formula)
  return(val)
  
}
#   
get.cell.value(cells, "AA65")


```

### Strategy for bringing in data

There are 2 approaches: 
1) Bring a single file in, extract all the cells/data of interest and populate a data frame. Then iterate over the list of files.
2) Go the purrr approach and import all files as a list column (the lists being tidyxl data frames), then extract values as columns into the data frame using mutate



