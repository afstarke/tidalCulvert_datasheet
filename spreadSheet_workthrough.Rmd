---
title: "Tidal spreadsheet workflow"
output: github_document

---
## Take 2:   
After some digging into the task of extracting data from these spreadsheets, and the prospects of additional analyses (prioritization sensitivity, perhaps) as well as easing the management of field and desktop assessment needs, a new(ish) approach is laid out below. In general a reboot of the teams initial proposal of using a sort of look up key to identify the locations of important data from within the spreadsheets.

To begin with, this process relies heavily on the tidyxl package, as well as the tidyverse packages more broadly. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

library(readr)
library(readxl)
library(tidyxl)
library(unpivotr)
library(tidyverse)

keysheet <- read_excel("../../../Box/Culvert Assessment/Tidal Assessments/key.xlsx") #Use the working more complete version

source("functions/culvert_tidy.R")
source("functions/culvert_extract.R")    

```

Using tidyxl's xlsx_cells() function we pull in all the cells from a given excel file. The cells arrive in a tidy data fashion where each row holds the data value, some formating information, data type, and the location in which it lives (the sheet and cell, specifically).  
Below is a snippet of a single sheet read into R using tidxl::xlsx_cells()

```{r}

# TODO Start with basic read in of spreadsheet and extracting the proper cells.
sheet <- "spreadsheets/Ex/Copy of Tidal Field Data blank.xlsm" 
# Tidy up the sheets. 
cells <- xlsx_cells(sheet) %>% 
  filter(is_blank == FALSE, sheet != "Data Sheet - BLANK") 
  
cells

```

The next step is to scale this up to capture all the datasheets on hand. We do this by creating two functions that work together. The first, **culvert_fetch()**, fetches the tidyxl cells when given a file path name. The second function, **culvert_tidy**, produces a data frame containing all the files within the supplied folder, a few helpful bits of info and a column containing the tidyxl cells from the output of **culvert_fetch()** for each data sheet. 



```{r message=FALSE, warning=FALSE, paged.print=FALSE}
culvert_fetch

culvert_tidy


test1 <- culvert_tidy("spreadsheets/Ex")

glimpse(test1)

glimpse(test1$tidycells[[1]])

```

From this we can start extracting the parts we want from the data sheet. To ease that process we'll rely on a pair of functions and a **_key_** spreadsheet for guiding the extraction of the bits we want.
To follow the process behind these next few operations we need to understand how things are coming together here. Essentially each row in the tidy culvert data frame contains a column with all the data from the original tidal culvert field assessment datasheets. Given the complexity of these datasheets and the lack of consistent orientations between data values and data keys (or data and variable name if you will) each item of interest needs to be mappped out individually using a key that provides the location within that spreadsheet that that value of interest is found (which if luck is on our side is found consistently through out all these files...Should be...) and a name that we want to give that value. 
Once the key is built (note the code will run at anytime and return only those values that are mapped out in the key) it's fed into a function that will pull out the 'raw' data form the spreadsheet (which now is that tidycells column) and pop it into an orderly, labelled, decoded sheet. And to ensure that nothing is mixed up in the process we'll add that new decoded data frame (or spreadsheet for sake of argument) as a new column to our already existing data frame. The result is a data frame that contains a row for each file that is found within the specified folder, the last time that file was altered, the path to the file, all the raw values from that spreadsheet, and lastly the decoded values from each culvert data sheet.  
From that we can then select the decoded values, and the filenames column (to act as the means of IDing which values came from which files if there's values that need to be QA'ed etc.) and flatten it out so that each row contains all the values of interest (which are identified in the key) into a tidy spreadsheet for downstream processing. 



```{r}


decodedSheet2 <- test1 %>% mutate(decoded = map(.x = tidycells, .f = ~decodeSheet(.x, keysheet))) # this is where the sausage is made. 


decodedSheet2 %>% 
  select(filenames, decoded) %>% 
  unnest() %>% select(filenames, dataName, values) %>% 
  spread(key = dataName, value = values) -> a

glimpse(a)


```

Approaching this problem in this way requires a few extra steps to recreate the formula and the assessment values that are calculated in the spreadsheet, but by tidying up the data in this way we can quickly alter-test-review the outputs of the assessment across ALL culverts assessed without having to individually reprogram each file.   
Below are some of my original ideas on the ups and downs of doing this which I'll leave in here for now.

#### Pros 
* More freedom and flexibility of up and down stream data management/analysis approaches, not limited to embedded excel formulas  
* If any sensitivity analysis or alterations to the prioritization calculations are to be explored those calculations can easily be altered; whereas now they exist in each individual excel file.
* At least in R, we can quickly find missing information from the 'raw' data sheet. Currently some formulas just yield more or less useless errors and are imported into R as '#Div/O!' 

#### Cons
* Will require a _fair amount_  of work re-coding Excel formulas into R code (or other)
  ** as mentioned above though most of the formulas are simple, single cell references, many even referencing cells that reference cells, that..etc. In fact of the 443 cells that contain some sort of formula only 139 of the cells contain references to more than one cell. And only a small proportion reference many cells (i.e. a bit messier / time consuming to dig into). In addition, many are simple _If(CELL = 0, " ", CELL)_- essentially changing just the way excel displays the information (blank vs 0)
  


### Potential pifalls
There's a few issues (potential) that I've come across.  

* cells are merged which makes it hard to determine which cell actually contains the value of interest. Seems as though it's the upper left-most cell in the set of merged cells that gives the proper address.   
* There are values that are selected using a formatted control (like a drop-down) which can be very easily altered.  
  + accessing these values is best done through the *Data Sheet - SUMMARY* tab.  Or perhaps just locked upon entering the data from the field sheets?
* There appear to be cells with just spaces possibly? They are showing up as blank, but NOT empty. I believe this is caused by IF(cell = 0, " ", cell) formulas.
* May have been mentioned above, dates are formatted differently it seems across and within spreadsheets. Maybe a setting in the author's version of excel? 

### TO DO's:  

* Figure out how to properly translate the dates and times from the format excel is using to R

 

### This could be handy if there's any cryptic values or formulas.

```{r}

# Helper function to look up values in a cell of interest.
# 

get.cell.value <- function(tidysheet, cellOfInterest){
  
  val <- tidysheet %>% 
    filter(address == cellOfInterest) %>% 
    pull(formula)
  return(val)
  
}
#   
get.cell.value(cells, "AA65")


get.cell.formula <- function(tidysheet, cellOfInterest){
  
  val <- tidysheet %>% 
    filter(address == cellOfInterest) %>% 
    select(formula) %>% as.character() %>% 
    xlex()
  return(val)
  
}
#   
get.cell.formula(cells, "AA64")

```

